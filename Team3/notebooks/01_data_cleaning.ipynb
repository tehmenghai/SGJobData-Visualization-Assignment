{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Cleaning\n",
    "\n",
    "**Team3 - Singapore Jobs Analytics**\n",
    "\n",
    "This notebook handles loading and cleaning the raw SGJobData dataset (~1M+ rows).\n",
    "\n",
    "**Objectives:**\n",
    "- Load the raw CSV dataset and verify its structure\n",
    "- Identify and remove empty/invalid rows (NULL IDs, placeholder salaries)\n",
    "- Analyze missing values and data quality issues\n",
    "- Export the cleaned dataset for downstream analysis\n",
    "\n",
    "**Tools:** DuckDB (in-memory OLAP), Pandas, Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB version: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Connect to DuckDB in-memory\n",
    "con = duckdb.connect(':memory:')\n",
    "print('DuckDB version:', duckdb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,048,585 rows into jobs_staging (raw, unfiltered)\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data into staging table (no filters — raw data as-is)\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE jobs_staging AS\n",
    "    SELECT * FROM read_csv_auto('../data/raw/SGJobData.csv', header=true, sample_size=-1)\n",
    "\"\"\")\n",
    "\n",
    "raw_count = con.execute('SELECT COUNT(*) FROM jobs_staging').fetchone()[0]\n",
    "print(f'Loaded {raw_count:,} rows into jobs_staging (raw, unfiltered)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset: 1,048,585 rows x 22 columns\n"
     ]
    }
   ],
   "source": [
    "# Verify raw row count and basic table info\n",
    "col_count = con.execute(\"SELECT COUNT(*) FROM information_schema.columns WHERE table_name = 'jobs_staging'\").fetchone()[0]\n",
    "print(f'Raw dataset: {raw_count:,} rows x {col_count} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Identify & Remove Empty/Invalid Rows\n",
    "\n",
    "Before analysis, we inspect the raw data for empty, placeholder, or invalid rows and clean them.\n",
    "This step ensures downstream analysis is not skewed by garbage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Empty / Invalid Row Analysis ===\n",
      "\n",
      "  NULL job ID (no identifier)                        3,988 rows (0.38%)\n",
      "  Empty title                                        3,988 rows (0.38%)\n",
      "  Empty company name                                 3,988 rows (0.38%)\n",
      "  Both salaries = 0                                  3,988 rows (0.38%)\n",
      "  Zero average salary                                3,988 rows (0.38%)\n",
      "  Placeholder salary (min=1, max=1, avg=1)           1,804 rows (0.17%)\n",
      "  Zero vacancies                                     3,988 rows (0.38%)\n",
      "  Empty categories                                   3,988 rows (0.38%)\n",
      "\n",
      "  Total raw rows                                 1,048,585\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Row Count</th>\n",
       "      <th>% of Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL job ID (no identifier)</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Empty title</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Empty company name</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both salaries = 0</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zero average salary</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Placeholder salary (min=1, max=1, avg=1)</td>\n",
       "      <td>1804</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zero vacancies</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Empty categories</td>\n",
       "      <td>3988</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Issue  Row Count  % of Total\n",
       "0               NULL job ID (no identifier)       3988        0.38\n",
       "1                               Empty title       3988        0.38\n",
       "2                        Empty company name       3988        0.38\n",
       "3                         Both salaries = 0       3988        0.38\n",
       "4                       Zero average salary       3988        0.38\n",
       "5  Placeholder salary (min=1, max=1, avg=1)       1804        0.17\n",
       "6                            Zero vacancies       3988        0.38\n",
       "7                          Empty categories       3988        0.38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Identify empty/invalid rows in the raw data\n",
    "print('=== Empty / Invalid Row Analysis ===\\n')\n",
    "\n",
    "checks = [\n",
    "    (\"metadata_jobPostId IS NULL\", \"NULL job ID (no identifier)\"),\n",
    "    (\"title IS NULL OR TRIM(title) = ''\", \"Empty title\"),\n",
    "    (\"postedCompany_name IS NULL OR TRIM(postedCompany_name) = ''\", \"Empty company name\"),\n",
    "    (\"salary_minimum = 0 AND salary_maximum = 0\", \"Both salaries = 0\"),\n",
    "    (\"average_salary = 0\", \"Zero average salary\"),\n",
    "    (\"salary_minimum = 1 AND salary_maximum = 1 AND average_salary = 1\", \"Placeholder salary (min=1, max=1, avg=1)\"),\n",
    "    (\"numberOfVacancies = 0\", \"Zero vacancies\"),\n",
    "    (\"categories IS NULL OR TRIM(categories) = '' OR categories = '[]'\", \"Empty categories\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for condition, label in checks:\n",
    "    cnt = con.execute(f\"SELECT COUNT(*) FROM jobs_staging WHERE {condition}\").fetchone()[0]\n",
    "    pct = cnt / raw_count * 100\n",
    "    results.append({'Issue': label, 'Row Count': cnt, '% of Total': round(pct, 2)})\n",
    "    print(f'  {label:45s} {cnt:>10,} rows ({pct:.2f}%)')\n",
    "\n",
    "print(f'\\n  {\"Total raw rows\":45s} {raw_count:>10,}')\n",
    "\n",
    "issues_df = pd.DataFrame(results)\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data quality issues\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plot_issues = issues_df[issues_df['Row Count'] > 0].sort_values('Row Count')\n",
    "\n",
    "colors = ['#EF4444' if cnt > raw_count * 0.01 else '#F59E0B' if cnt > 100 else '#10B981'\n",
    "          for cnt in plot_issues['Row Count']]\n",
    "bars = ax.barh(plot_issues['Issue'], plot_issues['Row Count'], color=colors)\n",
    "\n",
    "for bar, pct in zip(bars, plot_issues['% of Total']):\n",
    "    ax.text(bar.get_width() + raw_count * 0.003, bar.get_y() + bar.get_height()/2,\n",
    "            f'{int(bar.get_width()):,} ({pct}%)', va='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Number of Rows')\n",
    "ax.set_title('Data Quality Issues Found in Raw Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Inspect the empty rows — are they all the same set?\n",
    "print('=== Do the empty rows overlap? ===\\n')\n",
    "\n",
    "overlap = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) as cnt FROM jobs_staging\n",
    "    WHERE metadata_jobPostId IS NULL\n",
    "      AND (title IS NULL OR TRIM(title) = '')\n",
    "      AND salary_minimum = 0 AND salary_maximum = 0\n",
    "      AND (postedCompany_name IS NULL OR TRIM(postedCompany_name) = '')\n",
    "      AND numberOfVacancies = 0\n",
    "\"\"\").fetchone()[0]\n",
    "print(f'Rows matching ALL empty criteria simultaneously: {overlap:,}')\n",
    "print(f'Rows with NULL job ID:                           {con.execute(\"SELECT COUNT(*) FROM jobs_staging WHERE metadata_jobPostId IS NULL\").fetchone()[0]:,}')\n",
    "print(f'=> The {overlap:,} completely empty rows are the SAME set as NULL job ID rows.\\n')\n",
    "\n",
    "# Preview sample empty rows\n",
    "print('=== Sample of completely empty rows ===')\n",
    "sample_empty = con.execute(\"\"\"\n",
    "    SELECT title, postedCompany_name, salary_minimum, salary_maximum,\n",
    "           average_salary, numberOfVacancies, categories, metadata_jobPostId\n",
    "    FROM jobs_staging\n",
    "    WHERE metadata_jobPostId IS NULL\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "display(sample_empty)\n",
    "\n",
    "# Check placeholder salary rows (min=1, max=1) — these survive the NULL filter\n",
    "print('\\n=== Sample of placeholder salary rows (min=1, max=1, avg=1) ===')\n",
    "sample_placeholder = con.execute(\"\"\"\n",
    "    SELECT title, postedCompany_name, salary_minimum, salary_maximum,\n",
    "           average_salary, positionLevels, categories\n",
    "    FROM jobs_staging\n",
    "    WHERE metadata_jobPostId IS NOT NULL\n",
    "      AND salary_minimum = 1 AND salary_maximum = 1 AND average_salary = 1\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "display(sample_placeholder)\n",
    "\n",
    "placeholder_count = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) FROM jobs_staging\n",
    "    WHERE metadata_jobPostId IS NOT NULL\n",
    "      AND salary_minimum = 1 AND salary_maximum = 1 AND average_salary = 1\n",
    "\"\"\").fetchone()[0]\n",
    "print(f'\\nTotal placeholder salary rows: {placeholder_count:,} ({placeholder_count/raw_count*100:.2f}%)')\n",
    "print('These are real jobs where salary was not disclosed (SGD $1 is a placeholder).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean the data — remove empty rows and handle placeholder salaries\n",
    "print('=== Data Cleaning ===\\n')\n",
    "\n",
    "# Remove completely empty rows (NULL job ID) and placeholder salary rows\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE jobs_raw AS\n",
    "    SELECT * FROM jobs_staging\n",
    "    WHERE metadata_jobPostId IS NOT NULL          -- Remove 3,988 empty rows\n",
    "      AND NOT (salary_minimum = 1\n",
    "               AND salary_maximum = 1\n",
    "               AND average_salary = 1)            -- Remove placeholder salary rows\n",
    "\"\"\")\n",
    "\n",
    "cleaned_count = con.execute('SELECT COUNT(*) FROM jobs_raw').fetchone()[0]\n",
    "empty_removed = raw_count - cleaned_count\n",
    "placeholder_removed = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) FROM jobs_staging\n",
    "    WHERE metadata_jobPostId IS NOT NULL\n",
    "      AND salary_minimum = 1 AND salary_maximum = 1 AND average_salary = 1\n",
    "\"\"\").fetchone()[0]\n",
    "null_id_removed = raw_count - con.execute(\n",
    "    \"SELECT COUNT(*) FROM jobs_staging WHERE metadata_jobPostId IS NOT NULL\"\n",
    ").fetchone()[0]\n",
    "\n",
    "print(f'Cleaning steps applied:')\n",
    "print(f'  1. Removed rows with NULL job ID (completely empty): {null_id_removed:,} rows')\n",
    "print(f'  2. Removed placeholder salary rows (min=1, max=1, avg=1): {placeholder_removed:,} rows')\n",
    "print(f'  ---')\n",
    "print(f'  Total rows removed: {empty_removed:,} ({empty_removed/raw_count*100:.2f}%)')\n",
    "print(f'  Remaining rows:     {cleaned_count:,}')\n",
    "\n",
    "row_count = cleaned_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Verify cleaning — before vs after comparison\n",
    "print('=== Before vs After Cleaning ===\\n')\n",
    "\n",
    "verify_checks = [\n",
    "    (\"metadata_jobPostId IS NULL\", \"NULL job ID\"),\n",
    "    (\"title IS NULL OR TRIM(title) = ''\", \"Empty title\"),\n",
    "    (\"postedCompany_name IS NULL OR TRIM(postedCompany_name) = ''\", \"Empty company\"),\n",
    "    (\"salary_minimum = 0 AND salary_maximum = 0\", \"Both salaries = 0\"),\n",
    "    (\"salary_minimum = 1 AND salary_maximum = 1 AND average_salary = 1\", \"Placeholder salary (1/1/1)\"),\n",
    "    (\"average_salary = 0\", \"Zero average salary\"),\n",
    "    (\"numberOfVacancies = 0\", \"Zero vacancies\"),\n",
    "]\n",
    "\n",
    "print(f'{\"Check\":<40s} {\"Before\":>12s} {\"After\":>12s} {\"Removed\":>12s}')\n",
    "print('=' * 78)\n",
    "for cond, label in verify_checks:\n",
    "    before = con.execute(f\"SELECT COUNT(*) FROM jobs_staging WHERE {cond}\").fetchone()[0]\n",
    "    after = con.execute(f\"SELECT COUNT(*) FROM jobs_raw WHERE {cond}\").fetchone()[0]\n",
    "    print(f'{label:<40s} {before:>12,} {after:>12,} {before - after:>12,}')\n",
    "\n",
    "print('=' * 78)\n",
    "print(f'{\"TOTAL ROWS\":<40s} {raw_count:>12,} {cleaned_count:>12,} {raw_count - cleaned_count:>12,}')\n",
    "\n",
    "# Visualize before/after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Pie chart: kept vs removed\n",
    "removed = raw_count - cleaned_count\n",
    "axes[0].pie([cleaned_count, removed],\n",
    "            labels=[f'Kept\\n{cleaned_count:,}', f'Removed\\n{removed:,}'],\n",
    "            colors=['#10B981', '#EF4444'], autopct='%1.2f%%', startangle=90,\n",
    "            textprops={'fontsize': 11})\n",
    "axes[0].set_title('Data Cleaning: Rows Kept vs Removed')\n",
    "\n",
    "# Breakdown of removed rows\n",
    "removal_data = pd.DataFrame({\n",
    "    'Reason': ['Empty rows\\n(NULL job ID)', 'Placeholder salary\\n(min=max=avg=1)'],\n",
    "    'Count': [null_id_removed, placeholder_removed]\n",
    "})\n",
    "bars = axes[1].bar(removal_data['Reason'], removal_data['Count'], color=['#EF4444', '#F59E0B'])\n",
    "for bar in bars:\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                 f'{int(bar.get_height()):,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Rows Removed')\n",
    "axes[1].set_title('Breakdown of Removed Rows')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Summary\n",
    "\n",
    "**Raw data:** 1,048,585 rows loaded from `SGJobData.csv`\n",
    "\n",
    "**Rows removed:**\n",
    "1. **3,988 completely empty rows** — All fields are NULL/zero: no job ID, no title, no company, no salary. These are garbage rows with no usable information.\n",
    "2. **~1,700 placeholder salary rows** — `salary_minimum = 1`, `salary_maximum = 1`, `average_salary = 1`. These are real job postings where the employer chose not to disclose salary. The SGD $1 value is a system placeholder that would distort salary analysis.\n",
    "\n",
    "**Rows kept (not removed):**\n",
    "- **Zero views / zero applications** — Valid for newly posted or recently closed jobs. Removing them would bias engagement analysis.\n",
    "- **Zero experience required** — Valid for entry-level positions (10.96% of data).\n",
    "- **Salary min=1, max > 1** — Valid jobs with non-placeholder salaries (e.g., internships with $1 min and $1,000 max).\n",
    "\n",
    "**Cleaned dataset:** ~1,042,900 rows (99.5% retained) → stored in `jobs_raw` table for all downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls AND empty values for all columns (broader \"missing\" definition)\n",
    "schema = con.execute('DESCRIBE jobs_raw').fetchdf()\n",
    "columns = schema['column_name'].tolist()\n",
    "null_queries = [\n",
    "    f\"SUM(CASE WHEN \\\"{col}\\\" IS NULL OR CAST(\\\"{col}\\\" AS VARCHAR) = '' OR CAST(\\\"{col}\\\" AS VARCHAR) = '[]' THEN 1 ELSE 0 END) as \\\"{col}\\\"\"\n",
    "    for col in columns\n",
    "]\n",
    "null_sql = f\"SELECT {', '.join(null_queries)} FROM jobs_raw\"\n",
    "null_counts = con.execute(null_sql).fetchdf()\n",
    "\n",
    "null_df = null_counts.T.reset_index()\n",
    "null_df.columns = ['column', 'missing_count']\n",
    "null_df['missing_pct'] = (null_df['missing_count'] / row_count * 100).round(2)\n",
    "null_df = null_df.sort_values('missing_pct', ascending=True)\n",
    "\n",
    "print(f'Missing value check (NULL + empty string + empty JSON array):')\n",
    "print(f'Total rows: {row_count:,}\\n')\n",
    "null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart of missing percentages\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['#EF4444' if pct > 50 else '#F59E0B' if pct > 10 else '#10B981' for pct in null_df['missing_pct']]\n",
    "ax.barh(null_df['column'], null_df['missing_pct'], color=colors)\n",
    "ax.set_xlabel('% Missing (NULL + empty)')\n",
    "ax.set_title('Missing / Empty Values by Column')\n",
    "ax.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax.legend()\n",
    "\n",
    "for i, (pct, col) in enumerate(zip(null_df['missing_pct'], null_df['column'])):\n",
    "    ax.text(pct + 0.5, i, f'{pct}%', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Value distribution for key text fields\n",
    "print('\\n--- Unique value counts for key fields ---')\n",
    "for field in ['positionLevels', 'employmentTypes', 'salary_type', 'status_jobStatus']:\n",
    "    dist = con.execute(f\"\"\"\n",
    "        SELECT \"{field}\", COUNT(*) as cnt,\n",
    "               ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pct\n",
    "        FROM jobs_raw\n",
    "        GROUP BY \"{field}\"\n",
    "        ORDER BY cnt DESC\n",
    "    \"\"\").fetchdf()\n",
    "    print(f'\\n{field} ({len(dist)} unique values):')\n",
    "    print(dist.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Findings\n",
    "\n",
    "- **100% missing:** `occupationId` is entirely NULL across all 1M+ rows — this column is unusable and excluded from analysis.\n",
    "- **0% missing:** All other 21 columns are fully populated with no NULLs, empty strings, or empty JSON arrays.\n",
    "- **`positionLevels`** and **`employmentTypes`** are stored as plain strings (e.g., \"Executive\", \"Permanent\"), not JSON arrays — they are fully populated for every row.\n",
    "- **Salary fields:** `salary_minimum`, `salary_maximum`, and `average_salary` have no nulls.\n",
    "- **Core fields** (`metadata_jobPostId`, `title`, `postedCompany_name`) have zero nulls, ensuring strong coverage for primary analysis.\n",
    "- **Engagement metrics** (`views`, `applications`) are fully populated, enabling reliable engagement analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate job IDs\n",
    "duplicates = con.execute(\"\"\"\n",
    "    SELECT metadata_jobPostId, COUNT(*) as cnt\n",
    "    FROM jobs_raw\n",
    "    GROUP BY metadata_jobPostId\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f'Duplicate job IDs: {len(duplicates):,}')\n",
    "if len(duplicates) > 0:\n",
    "    print(f'Total duplicate rows: {duplicates[\"cnt\"].sum() - len(duplicates):,}')\n",
    "    print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalid salary ranges (max < min)\n",
    "invalid_salary = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) as invalid_count,\n",
    "           ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM jobs_raw WHERE salary_minimum IS NOT NULL AND salary_maximum IS NOT NULL), 2) as pct\n",
    "    FROM jobs_raw\n",
    "    WHERE salary_maximum < salary_minimum\n",
    "      AND salary_minimum IS NOT NULL\n",
    "      AND salary_maximum IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "print('Invalid salary ranges (max < min):')\n",
    "print(invalid_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty JSON fields check\n",
    "json_quality = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN categories IS NULL OR categories = '' OR categories = '[]' THEN 1 ELSE 0 END) as empty_categories,\n",
    "        SUM(CASE WHEN positionLevels IS NULL OR positionLevels = '' OR positionLevels = '[]' THEN 1 ELSE 0 END) as empty_position_levels,\n",
    "        SUM(CASE WHEN employmentTypes IS NULL OR employmentTypes = '' OR employmentTypes = '[]' THEN 1 ELSE 0 END) as empty_employment_types,\n",
    "        COUNT(*) as total\n",
    "    FROM jobs_raw\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "json_quality_pct = json_quality.copy()\n",
    "total = json_quality['total'].iloc[0]\n",
    "for col in ['empty_categories', 'empty_position_levels', 'empty_employment_types']:\n",
    "    json_quality_pct[col] = (json_quality[col] / total * 100).round(2)\n",
    "print('Empty JSON fields (% of total rows):')\n",
    "json_quality_pct[['empty_categories', 'empty_position_levels', 'empty_employment_types']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary type distribution\n",
    "salary_types = con.execute(\"\"\"\n",
    "    SELECT salary_type, COUNT(*) as cnt,\n",
    "           ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pct\n",
    "    FROM jobs_raw\n",
    "    GROUP BY salary_type\n",
    "    ORDER BY cnt DESC\n",
    "\"\"\").fetchdf()\n",
    "print('Salary type distribution:')\n",
    "salary_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Export Cleaned Data\n",
    "\n",
    "Export the cleaned dataset to `data/processed/` for use by downstream notebooks (EDA, Feature Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to parquet\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    COPY jobs_raw TO '../data/processed/jobs_cleaned.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "# Verify export\n",
    "exported_count = con.execute(\"SELECT COUNT(*) FROM read_parquet('../data/processed/jobs_cleaned.parquet')\").fetchone()[0]\n",
    "print(f'Exported {exported_count:,} rows to ../data/processed/jobs_cleaned.parquet')\n",
    "print(f'Matches cleaned count: {exported_count == cleaned_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "con.close()\n",
    "print('Data cleaning complete. Cleaned data exported to data/processed/jobs_cleaned.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
